{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Introduction\n",
    "\n",
    "*INSERT LINK TO WANDB VIEW*"
   ],
   "id": "d4cbec299a5d4f64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-07T16:25:43.192141Z",
     "start_time": "2025-03-07T16:25:43.181299Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3db4c54b33606e47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Setup",
   "id": "dd1fcc78251ae773"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Install dependencies\n",
    "\n",
    "* **torch**: PyTorch framework for the creation of neural networks\n",
    "* **lightning**: Lightning wrapper for pytorch for simple network training\n",
    "* **huggingface_hub**: HuggingFace hub for downloading word vectors\n",
    "* **datasets**: HuggingFace datasets to download and load the data set\n",
    "* **wandb**: Weights & Biases for experiment tracking\n",
    "* **fasttext**: Word embedding library\n",
    "* **nltk**: Natural Language Toolkit used for word tokenization\n",
    "* **evaluate**: ..."
   ],
   "id": "ae74b556f3aff49"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T13:14:25.215237Z",
     "start_time": "2025-03-09T13:14:21.829472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "if sys.platform == 'win32': # Windows requires different fasttext implementation\n",
    "    %pip install -q torch lightning huggingface_hub datasets wandb fasttext-wheel nltk evaluate\n",
    "else: \n",
    "    %pip install -q torch lightning huggingface_hub datasets wandb fasttext nltk evaluate"
   ],
   "id": "33f5c188a84c79a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Load dataset\n",
    "\n",
    "Use the pre-defined method to load the dataset and do the train and validation split"
   ],
   "id": "b696ef4250c380a0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T13:40:47.657111Z",
     "start_time": "2025-03-09T13:40:37.046099Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "train: Dataset = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid: Dataset = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")\n",
    "test: Dataset = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ],
   "id": "ad899724a20482d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8741 1000 1221\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Setup Weights & Biases\n",
    "\n",
    "Login to weights and biases to enable experiment tracking for later network training"
   ],
   "id": "cdc78217c5ed1f84"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "wandb.login()"
   ],
   "id": "b9bc41bae4172979",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "\n",
    "#### Vocabulary/Embedding\n",
    "\n",
    "* I decided to use the **FastText** library for this project, since in class it was said that FastText is superior to the other embedding models and there is no problem with embedding unknown words, because it can create word vectors from their subwords. Furthermore, I will be working with the **facebook/fasttext-en-vectors** word vectors from the HuggingFace hub. They embed words from the English language, which is the only relevant language.\n",
    "\n",
    "* This choice influences decisions in the following pre-processing steps.\n",
    "\n",
    "#### Format cleaning (e.g. html-extracted text)\n",
    "\n",
    "* No format cleaning is performed, because we work with a carefully assembled and standardized dataset used in model benchmarking.\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "* *word_tokenizer* from the **nltk** library will be used. This tokenizer works well for the English language. It also splits punctuation from text, which matches the tokens the fasttext word vectors were trained on.\n",
    "\n",
    "#### Lowercasing, stemming, lemmatizing, stopword/punctuation removal\n",
    "\n",
    "* **Lowercasing**: Although the word vectors in use were trained on case-sensitive data, the tokenized words will be lowercased to reach a smaller vocabulary and minimize out-of-vocabulary words.\n",
    "* **Stemming**: The word embedding model was not trained on word stems and therefore no stemming is carried out.\n",
    "* **Lemmatizing**: The word tokens to be embedded will not be lemmatized, because the fasttext model was trained on un-lemmatized words and the n-gram encoding of the words used in fasttext preserves sub-word information.\n",
    "* **Stopword/Punctudation removal**: Since the task is to answer common-sense questions, stopwords and punctuation will not be removed. Most of the questions are quite short and the loss of information if either a critical stopword in the question or punctuation that changes the meaning of the question is removed could be significant.\n",
    "\n",
    "#### Removal of unknown/other words\n",
    "\n",
    "* Since I am working with a fasttext model, the removal of unknown words is not necessary, because vectors for them can implicitly be built from their n-gram vectors. Also, the encounter of unknown words is not expected.\n",
    "\n",
    "#### Truncation\n",
    "\n",
    "* \n",
    "\n",
    "#### Feature selection\n",
    "\n",
    "* \n",
    "\n",
    "#### Input format: how is data passed to the model?\n",
    "\n",
    "* \n",
    "\n",
    "#### Label format: what should the model predict?\n",
    "\n",
    "* \n",
    "\n",
    "#### Train/valid/test splits\n",
    "\n",
    "* As seen in the *Introduction* section, the train/validation/test splits are performed as defined in the course\n",
    "\n",
    "#### Batching, padding\n",
    "\n",
    "* TODO: test on gpu-hub for optimal batching. padding only necessary for RNN model?"
   ],
   "id": "b635a3413d69a3fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenize\n",
    "\n",
    "Create method to tokenize and lowercase a given text"
   ],
   "id": "8bf5eb8363eacb78"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    return [w.lower() for w in nltk.word_tokenize(text, language=\"english\")]"
   ],
   "id": "5b71c0f47430aebe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Word embeddings\n",
    "\n",
    "Download the english fasttext word vectors and load their model into the variable *wv_model*\n",
    "\n",
    "Create a function to embed a list of tokenized words and return them as a list of pytorch tensors"
   ],
   "id": "226d42f6a99035f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T13:54:08.825845Z",
     "start_time": "2025-03-09T13:53:48.991537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(\"facebook/fasttext-en-vectors\", \"model.bin\")\n",
    "wv_model = fasttext.load_model(model_path)\n",
    "\n",
    "from torch import tensor\n",
    "\n",
    "def get_embeddings_for_tokens(tokens: list[str]):\n",
    "    return [tensor(wv_model[t]) for t in tokens]\n",
    "    "
   ],
   "id": "1403d38e9ae55f30",
   "outputs": [],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Data Loading and Formatting\n",
    "\n",
    "Create a **pytorch** *Dataset* class in which the HuggingFace dataset is loaded and preprocessed. This allows for an easy integration with a *DataLoader* afterward."
   ],
   "id": "762f05a64aba4571"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-09T14:22:42.207291Z",
     "start_time": "2025-03-09T14:22:42.191471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "KEY_INDEX_MAPPING = {\n",
    "    \"A\": 0,\n",
    "    \"B\": 1,\n",
    "    \"C\": 2,\n",
    "    \"D\": 3,\n",
    "    \"E\": 4,\n",
    "}\n",
    "\n",
    "class CommonsenseQADataset(Dataset):    \n",
    "    def __init__(self, dataset: HFDataset):\n",
    "        self.dataset: list[dict[str, torch.tensor | list[torch.tensor]]] = []\n",
    "        self._transform_hugging_face_dataset(dataset)\n",
    "    \n",
    "    def _transform_hugging_face_dataset(self, dataset: HFDataset):\n",
    "        for entry in dataset:\n",
    "            self.dataset.append({\n",
    "                \"question\": get_embeddings_for_tokens(tokenize(entry[\"question\"])),\n",
    "                \"choices\": [get_embeddings_for_tokens(tokenize(choice)) for choice in entry[\"choices\"][\"text\"]],\n",
    "                \"answer\": torch.eye(5)[KEY_INDEX_MAPPING[entry[\"answerKey\"]]],\n",
    "            })\n",
    "        if len(self.dataset) != len(dataset):\n",
    "            raise RuntimeError(\"Converted dataset is not full reflection of source data\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pass"
   ],
   "id": "beff391944e99b90",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model",
   "id": "1a26a4c41b4d0171"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "62e9d4d4e8704f92",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training",
   "id": "4c1d04ba2f9898ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "f20c68b42f1e3715",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Evaluation",
   "id": "78a92ceca9fa4119"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "24831257099358aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Interpretation",
   "id": "3bb349b7a3afb091"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c163ba3cc6c0116d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
