{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4cbec299a5d4f64",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "* # TODO: INSERT LINK TO WANDB VIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db4c54b33606e47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T06:15:17.466986Z",
     "start_time": "2025-03-19T06:15:17.462621Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd1fcc78251ae773",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae74b556f3aff49",
   "metadata": {},
   "source": [
    "#### Install dependencies\n",
    "\n",
    "* **torch**: PyTorch framework for the creation of neural networks\n",
    "* **lightning**: Lightning wrapper for pytorch for simple network training\n",
    "* **huggingface_hub**: HuggingFace hub for downloading word vectors\n",
    "* **datasets**: HuggingFace datasets to download and load the data set\n",
    "* **wandb**: Weights & Biases for experiment tracking\n",
    "* **fasttext**: Word embedding library\n",
    "* **nltk**: Natural Language Toolkit used for word tokenization\n",
    "* **torchmetrics**: Extension to lightning to compute model metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33f5c188a84c79a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T09:27:44.467950Z",
     "start_time": "2025-03-19T09:27:40.158949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "%pip install -q torch lightning huggingface_hub datasets wandb nltk torchmetrics\n",
    "\n",
    "if sys.platform == 'win32': # Windows requires different fasttext implementation\n",
    "    %pip install -q fasttext-wheel\n",
    "else: \n",
    "    %pip install -q fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b696ef4250c380a0",
   "metadata": {},
   "source": [
    "#### Load dataset\n",
    "\n",
    "Use the pre-defined method to load the dataset and do the train and validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad899724a20482d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T12:49:59.439696Z",
     "start_time": "2025-03-19T12:49:42.466851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf2a7b8fd05f486b828cf5a008cdfb5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.39k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30db924c59564a39a72f397e02d1f43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/1.25M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "876f6822c51d47268f452029685a2521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/160k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d644c89342e476d87cd4c32133fbfb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/151k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e2e2ba7b0ee4b69bd77de88955c470e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/9741 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a736e60081c04c25bd65595b063b6167",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa01db225d14d32a5616558d4865025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1140 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8741 1000 1221\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "train: Dataset = load_dataset(\"tau/commonsense_qa\", split=\"train[:-1000]\")\n",
    "valid: Dataset = load_dataset(\"tau/commonsense_qa\", split=\"train[-1000:]\")\n",
    "test: Dataset = load_dataset(\"tau/commonsense_qa\", split=\"validation\")\n",
    "\n",
    "print(len(train), len(valid), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc78217c5ed1f84",
   "metadata": {},
   "source": [
    "#### Setup Weights & Biases\n",
    "\n",
    "Login to weights and biases to enable experiment tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9bc41bae4172979",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/jovyan/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mschurtenberger-david\u001b[0m (\u001b[33mdavid-schurtenberger\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b635a3413d69a3fc",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "#### Vocabulary/Embedding\n",
    "\n",
    "* I decided to use the **FastText** library for this project, since in class it was said that FastText is superior to the other embedding models and there is no problem with embedding unknown words, because it can create word vectors from their subwords. Furthermore, I will be working with the **facebook/fasttext-en-vectors** word vectors from the HuggingFace hub. They embed words from the English language, which is the only relevant language.\n",
    "\n",
    "* This choice influences decisions in the following pre-processing steps.\n",
    "\n",
    "#### Format cleaning (e.g. html-extracted text)\n",
    "\n",
    "* No format cleaning is performed, because we work with a carefully assembled and standardized dataset used in model benchmarking.\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "* *word_tokenizer* from the **nltk** library will be used. This tokenizer works well for the English language. It also splits punctuation from text, which matches the tokens the fasttext word vectors were trained on.\n",
    "\n",
    "#### Lowercasing, stemming, lemmatizing, stopword/punctuation removal\n",
    "\n",
    "* **Lowercasing**: Although the word vectors in use were trained on case-sensitive data, the tokenized words will be lowercased to reach a smaller vocabulary and minimize out-of-vocabulary words.\n",
    "* **Stemming**: The word embedding model was not trained on word stems and therefore no stemming is carried out.\n",
    "* **Lemmatizing**: The word tokens to be embedded will not be lemmatized, because the fasttext model was trained on un-lemmatized words and the n-gram encoding of the words used in fasttext preserves sub-word information.\n",
    "* **Stopword/Punctudation removal**: Since the task is to answer common-sense questions, stopwords and punctuation will not be removed. Most of the questions are quite short and the loss of information if either a critical stopword in the question or punctuation that changes the meaning of the question is removed could be significant.\n",
    "\n",
    "#### Removal of unknown/other words\n",
    "\n",
    "* Since I am working with a fasttext model, the removal of unknown words is not necessary, because vectors for them can implicitly be built from their n-gram vectors. Also, the encounter of unknown words is not expected.\n",
    "\n",
    "#### Truncation\n",
    "\n",
    "* Input will not be truncated. After some data review, the question yielding the most embedded word vectors yields a tensor of shape **300x67**. Depending on the input format, the RNN model will have to perform significantly less than 100 time steps for the longest input, which is deemed to be feasible. Also, if padding is implemented correctly, for every timestep only the necessary amount of time steps will be executed.\n",
    "\n",
    "#### Feature selection\n",
    "\n",
    "* Of the available features, the **question**, the **choices** and the **answerKey** were chosen. While the *questionConcept* seemed like an interesting feature at first, after some data review it was determined, that this feature often simply contains a word from the question. In the end this feature was left out in order not to give too much emphasis to a single word that does likely not help answering the question at all.\n",
    "\n",
    "#### Input format: how is data passed to the model?\n",
    "\n",
    "###### Classifier\n",
    "\n",
    "   * I chose the input for the *Classifier Model* to be a tensor of size **1800**. The first 300 elements are the averages of the embedded question tokens, next are 300 elements for every embedded and averaged answer vector from answer option 'A' to 'E'.\n",
    "      * The average of the question vectors was chosen, because it is a good tradeoff between information retention and input  dimension for the classifier.\n",
    "      * The question vector is before the answer vectors, because \"Q&A\" also has question first, then answers.\n",
    "      * The answers are arranged from 'A' to 'E' because of alphabetical order.\n",
    "      * The average of the answer embeddings has been chosen, since answers can consist of multiple words and therefore may yield multiple embedding vectors.\n",
    "\n",
    "###### RNN + Classifier\n",
    "\n",
    "   * I chose the input for the *RNN + Classifier Model* to be tensor of size **300 x (N + 10)**. The first *N* columns of the tensor are the word-embeddings of the question. The last *10* columns are the averages of the word embeddings for each answer choice, separated by a *SEP* token.\n",
    "      * As separation token the character **¦** was chosen, because it is known to the embeddings model, but does not appear in the data.\n",
    "      * The separation token was introduced to signal to the model, that after the input after this reserved token is an answer choice.\n",
    "      * All answer embeddings are concatenated to the question embeddings to only need one full pass through the RNN model to get a prediction.\n",
    "      * The answer embeddings are after the question embeddings, because \"Q&A\" also has question first, then answer.\n",
    "\n",
    "#### Label format: what should the model predict?\n",
    "\n",
    "* Both model architectures will predict a vector of length 5. Every answer choice ('A' through 'E') is encoded on an index in the vector (0 through 4). Since the classifier at the last stage of the model predicts the likelihood of each output, this output format seems the most reasonable. Also, with this kind of output, the model only needs to run once for every classification, which should increase compute performance.\n",
    "\n",
    "#### Train/valid/test splits\n",
    "\n",
    "* As seen in the *Introduction* section, the train/validation/test splits are performed as defined in the course\n",
    "\n",
    "#### Batching, padding\n",
    "\n",
    "* # TODO: test on gpu-hub for optimal batching. padding only necessary for RNN model?\n",
    "* For each model, \n",
    "* Only the input of the **RNN + Classifier** model is of variable size, therefore padding is only necessary for that model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf5eb8363eacb78",
   "metadata": {},
   "source": [
    "### Tokenize\n",
    "\n",
    "Create method to tokenize and lowercase a given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b71c0f47430aebe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T12:50:12.719442Z",
     "start_time": "2025-03-19T12:50:12.219600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "\n",
    "def tokenize(text: str) -> list[str]:\n",
    "    return [w.lower() for w in nltk.word_tokenize(text, language=\"english\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d42f6a99035f0",
   "metadata": {},
   "source": [
    "### Word embeddings\n",
    "\n",
    "Download the english fasttext word vectors and load their model into the variable *wv_model*\n",
    "\n",
    "Create a function to embed a list of tokenized words and return them as a list of pytorch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1403d38e9ae55f30",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T12:50:22.598266Z",
     "start_time": "2025-03-19T12:50:14.968565Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a7de380a3340eca3807ed06f076eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.bin:   0%|          | 0.00/7.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import fasttext\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "model_path = hf_hub_download(\"facebook/fasttext-en-vectors\", \"model.bin\")\n",
    "wv_model = fasttext.load_model(model_path)\n",
    "\n",
    "def get_embeddings_for_tokens(tokens: list[str]):\n",
    "    return torch.stack([torch.tensor(wv_model[t]) for t in tokens]).T\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762f05a64aba4571",
   "metadata": {},
   "source": [
    "### Data Loading and Formatting\n",
    "\n",
    "Create a **pytorch** *Dataset* class in which the HuggingFace dataset is loaded and preprocessed. This allows for an easy integration with a *DataLoader* afterward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2dfba7d95d0678",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T12:50:25.062162Z",
     "start_time": "2025-03-19T12:50:23.659421Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "TransformMethod = Callable[[torch.Tensor, list[torch.Tensor]], torch.Tensor]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa78299e9e366c1d",
   "metadata": {},
   "source": [
    "Create a separator token from a character that is known to the word vector model, but unused in the train, valid and test datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40a64f78521705ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T12:50:29.743454Z",
     "start_time": "2025-03-19T12:50:29.143921Z"
    }
   },
   "outputs": [],
   "source": [
    "def char_not_in_huggingface_dataset(char: str, dataset: HFDataset) -> bool:\n",
    "    for datapoint in dataset:\n",
    "        if char in datapoint[\"question\"] or any(char in c for c in datapoint[\"choices\"][\"text\"]):\n",
    "            return False\n",
    "    return True\n",
    "    \n",
    "\n",
    "placeholder = \"¦\"\n",
    "assert placeholder in wv_model # Check if placeholder is a known token in the model\n",
    "assert char_not_in_huggingface_dataset(placeholder, train)\n",
    "assert char_not_in_huggingface_dataset(placeholder, valid)\n",
    "assert char_not_in_huggingface_dataset(placeholder, test)\n",
    "\n",
    "SEP_TOKEN = torch.tensor(wv_model[placeholder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "beff391944e99b90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T12:50:32.939190Z",
     "start_time": "2025-03-19T12:50:32.926706Z"
    }
   },
   "outputs": [],
   "source": [
    "KEY_INDEX_MAPPING = {\n",
    "    \"A\": 0,\n",
    "    \"B\": 1,\n",
    "    \"C\": 2,\n",
    "    \"D\": 3,\n",
    "    \"E\": 4,\n",
    "}\n",
    "\n",
    "class CommonsenseQADataset(Dataset):\n",
    "    _target_transform: TransformMethod\n",
    "    \n",
    "    def __init__(self, dataset: HFDataset):\n",
    "        self.dataset: list[dict[str, torch.tensor | list[torch.tensor]]] = []\n",
    "        self._transform_hugging_face_dataset(dataset)\n",
    "        \n",
    "    def set_target_transform(self, transform: TransformMethod):\n",
    "        self._target_transform = transform\n",
    "    \n",
    "    def _transform_hugging_face_dataset(self, dataset: HFDataset):\n",
    "        self.dataset.extend([{\n",
    "            \"question\": get_embeddings_for_tokens(tokenize(entry[\"question\"])),\n",
    "            \"choices\": torch.hstack([get_embeddings_for_tokens(tokenize(choice)).mean(dim=1).unsqueeze(1) for choice in entry[\"choices\"][\"text\"]]),\n",
    "            \"answer\": torch.eye(5)[KEY_INDEX_MAPPING[entry[\"answerKey\"]]],\n",
    "        } for entry in dataset])\n",
    "        if len(self.dataset) != len(dataset):\n",
    "            raise RuntimeError(\"Converted dataset is not full reflection of source data\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data_point = self.dataset[idx]\n",
    "        feature = self._target_transform(data_point[\"question\"], data_point[\"choices\"])\n",
    "        target = data_point[\"answer\"]\n",
    "        return feature, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cc3469965c8b86",
   "metadata": {},
   "source": [
    "Interchangeable transform function for **Classifier** network\n",
    "\n",
    "* Function input: Question-tensor (300 x N) and answer-tensor (300 x 5)\n",
    "* Function output: Classifier vector (1800)\n",
    "    * Average of question vectors yields vector of size 300\n",
    "    * Question vector and answer vectors are concatenated (5 * 300 = 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1384f3a7738a244",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T12:50:36.052757Z",
     "start_time": "2025-03-19T12:50:36.042344Z"
    }
   },
   "outputs": [],
   "source": [
    "def classifier_target_transform(question: torch.Tensor, answers: torch.Tensor) -> torch.Tensor:\n",
    "    return torch.cat((question.mean(dim=1), answers.T.flatten()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b65a982633864dd",
   "metadata": {},
   "source": [
    "Interchangeable transform function for **RNN + Classifier** network\n",
    "\n",
    "* Function input: Question-tensor (300 x N) and answer-tensor (300 x 5)\n",
    "* Function output: RNN-tensor (300 x (N + 10)) with N question vectors, 5 answer vectors and 5 SEP_TOKEN vectors separating the question from the answer and the answers from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7278eaffa9f7bae1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T12:50:37.480585Z",
     "start_time": "2025-03-19T12:50:37.468410Z"
    }
   },
   "outputs": [],
   "source": [
    "def rnn_target_transform(question: torch.Tensor, answers: torch.Tensor) -> torch.Tensor:\n",
    "    separated_answers = torch.empty(300, 10)\n",
    "    for i in range(5):\n",
    "        separated_answers[:, 2*i] = SEP_TOKEN\n",
    "        separated_answers[:, 2*i+1] = answers[:, i]\n",
    "    return torch.cat([question, separated_answers], dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4941497bdf4f65",
   "metadata": {},
   "source": [
    "Transform HuggingFace datasets to pytorch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ae26b8c01da94b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T13:40:42.674222Z",
     "start_time": "2025-03-19T13:40:35.905221Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = CommonsenseQADataset(train)\n",
    "valid_data = CommonsenseQADataset(valid)\n",
    "test_data = CommonsenseQADataset(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554586163be5eb6b",
   "metadata": {},
   "source": [
    "# 1. Architecture: WordEmbeddings &rarr; Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a26a4c41b4d0171",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "I chose to use **lightning** to create a streamlined model training process. The *LightningModule* subclass was created with the help of the [API doc](https://lightning.ai/docs/pytorch/LTS/common/lightning_module.html#lightningmodule-api) and the \"experiment_tracking\" notebook that we looked at in the Project Discussion lecture\n",
    "\n",
    "The model architecture complies with the required architecture in the project description.\n",
    "* Between input and hidden layer there is RELU-non-linearity as activation function. Reason: required\n",
    "* The output of the second layer is activated using SoftMax. Reason: meaningful output activation for multiclass classification\n",
    "* The metrics \"val_loss\", \"val_acc\", \"train_loss\", \"train_acc\" are logged after every epoch. Reason: Meaningful metrics, not overwhelming experiment tracking view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62e9d4d4e8704f92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T12:51:00.219715Z",
     "start_time": "2025-03-19T12:50:59.087784Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torchmetrics\n",
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9b28aac91bda610e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T12:51:01.362014Z",
     "start_time": "2025-03-19T12:51:01.348628Z"
    }
   },
   "outputs": [],
   "source": [
    "class CqaClassifier(L.LightningModule):\n",
    "    def __init__(\n",
    "            self, \n",
    "            input_dim: int = 1800, \n",
    "            hidden_dim: int = 4096, \n",
    "            output_dim: int = 5, \n",
    "            learning_rate: float = 1e-4,\n",
    "            adam_epsilon: float = 1e-8,\n",
    "            weight_decay: float = 0.0,          \n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        self._train_acc = torchmetrics.Accuracy(\"multiclass\", num_classes=output_dim)\n",
    "        self._train_loss = []\n",
    "        self._valid_acc = torchmetrics.Accuracy(\"multiclass\", num_classes=output_dim)\n",
    "        self._valid_loss = []\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        fc1 = torch.relu(self.fc1(x))\n",
    "        output = torch.softmax(self.fc2(fc1), dim=1)\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self._train_loss.append(loss)\n",
    "        self._train_acc(y_hat, y)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self._valid_loss.append(loss)\n",
    "        self._valid_acc(y_hat, y)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = self.loss_fn(y_hat, y)\n",
    "        self.log('test_loss', loss)\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        loss = torch.stack(self._train_loss).mean()\n",
    "        self.log_dict({'train_loss': loss, 'train_acc': self._train_acc.compute()}, prog_bar=True)\n",
    "        self._train_loss.clear()\n",
    "        self._train_acc.reset()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        loss = torch.stack(self._valid_loss).mean()\n",
    "        self.log_dict({'valid_loss': loss, 'valid_acc': self._valid_acc.compute()}, prog_bar=True)\n",
    "        self._train_loss.clear()\n",
    "        self._valid_acc.reset()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            self.parameters(), \n",
    "            lr=self.hparams.learning_rate, \n",
    "            eps=self.hparams.adam_epsilon, \n",
    "            weight_decay=self.hparams.weight_decay\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1d04ba2f9898ed",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ad86e7de95bd58",
   "metadata": {},
   "source": [
    "#### Utilities\n",
    "\n",
    "Create a Utility class for run parameters, that automatically creates a meaningful run-name and returns the config for wandb\n",
    "\n",
    "* Require a model_name, this will also be used as run-name on wandb. Reason: directly clear what model is trained, but not too much information in run name as recommended from wandb\n",
    "* Optional hyperparameter *kwargs* that are passed to the wandb initialization as config. Reason: Track all hyperparameters for reproducability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f80f88f66034749",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T12:51:04.576927Z",
     "start_time": "2025-03-19T12:51:04.567358Z"
    }
   },
   "outputs": [],
   "source": [
    "class RunParameters:\n",
    "    def __init__(self, model_name: str, **kwargs):\n",
    "        self._name = model_name\n",
    "        self._params = dict(kwargs)\n",
    "    \n",
    "    def __call__(self) -> dict:\n",
    "        return self._params\n",
    "    \n",
    "    def __str__(self):\n",
    "        return self._name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf00949e4a7c6540",
   "metadata": {},
   "source": [
    "Utility function to find the optimal batch size. A batch size that uses 80% of the available GPU memory was deemed to be optimal. Reason: Use bulk of memory available, but leave some headroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "336b81312efd5aff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T13:47:15.315461Z",
     "start_time": "2025-03-19T13:47:15.306884Z"
    }
   },
   "outputs": [],
   "source": [
    "def batch_size_finder(data: Dataset, model: L.LightningModule, max_memory_usage=0.8) -> tuple[int, float]:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if not device == \"cuda\":\n",
    "        raise RuntimeError(\"Can only be run on gpu\")\n",
    "    batch_size, memory_usage = 1, 0\n",
    "    model.to(device)\n",
    "    \n",
    "    while True:\n",
    "        loader = torch.utils.data.DataLoader(data, batch_size=batch_size)\n",
    "        inputs = labels = torch.tensor([])\n",
    "        for i, l in loader:\n",
    "            if i.shape > inputs.shape:\n",
    "                inputs, labels = i, l\n",
    "        try:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            _ = model(inputs)\n",
    "            \n",
    "            # Check memory usage\n",
    "            mem_allocated = torch.cuda.memory_allocated(device)\n",
    "            mem_reserved = torch.cuda.memory_reserved(device)\n",
    "            mem_usage = mem_allocated / mem_reserved\n",
    "\n",
    "            print(f\"Batch Size: {batch_size}, Memory Usage: {mem_usage:%}\")\n",
    "\n",
    "            if mem_usage <= memory_usage:\n",
    "                return batch_size // 2, memory_usage\n",
    "            \n",
    "            if mem_usage >= max_memory_usage:\n",
    "                return batch_size, mem_usage\n",
    "           \n",
    "            batch_size *= 2\n",
    "            memory_usage = mem_usage\n",
    "        \n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e):\n",
    "                return batch_size // 2, memory_usage\n",
    "            raise e\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302e5be5719640b0",
   "metadata": {},
   "source": [
    "#### Run parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250550d4a74675ae",
   "metadata": {},
   "source": [
    "Set the data target transformer to the Classifier model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e5d682f2c543bae3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T13:47:17.221887Z",
     "start_time": "2025-03-19T13:47:17.215960Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data.set_target_transform(classifier_target_transform)\n",
    "valid_data.set_target_transform(classifier_target_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda600335fa7906",
   "metadata": {},
   "source": [
    "Find optimal batch size for a model with the given amount of hidden layers on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b351e436e6bfdd5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T13:47:18.245667Z",
     "start_time": "2025-03-19T13:47:18.179844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Size: 1, Memory Usage: 90.755547%\n",
      "Batch Size: 1 @ 90.755547% memory usage\n"
     ]
    }
   ],
   "source": [
    "HIDDEN_DIM = 4096\n",
    "BATCH_SIZE, memory_usage = batch_size_finder(train_data, CqaClassifier(hidden_dim=HIDDEN_DIM))\n",
    "print(f\"Batch Size: {BATCH_SIZE} @ {memory_usage:%} memory usage\")\n",
    "BATCH_SIZE = 256 # Overwrite batch size, because entire dataset could be loaded at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "20fced576b1c3b20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T13:35:13.115411Z",
     "start_time": "2025-03-19T13:35:13.110073Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = RunParameters(\n",
    "    \"Classifier-v1\",\n",
    "    learning_rate=1e-4,\n",
    "    epochs=20,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    adam_epsilon=1e-8,\n",
    "    weight_decay=0.0,\n",
    "    train_batch_size=BATCH_SIZE,\n",
    "    valid_batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4911a7dab1bb10",
   "metadata": {},
   "source": [
    "#### Initialize wandb experiment tracking for run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82b2dd9acb4f7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7575c2ac5ad19d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    entity=\"david-schurtenberger\",\n",
    "    project=\"NLP_Project_1\",\n",
    "    name=str(parameters),\n",
    "    config=parameters(),\n",
    ")\n",
    "wandb_logger = WandbLogger(project=\"NLP_Project_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2783a8e580d74f72",
   "metadata": {},
   "source": [
    "#### Training routine\n",
    "\n",
    "Define the function **train_classifier** which describes the training routine for the classifier model using the **Trainer** class of *lightning* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e462b206a5f1b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(config, logger, train_loader, valid_loader):\n",
    "    L.seed_everything(42)\n",
    "    model = CqaClassifier(\n",
    "        hidden_dim=config.get(\"hidden_dim\"),\n",
    "        learning_rate=config.get(\"learning_rate\"),\n",
    "        adam_epsilon=config.get(\"adam_epsilon\"),\n",
    "        weight_decay=config.get(\"weight_decay\")\n",
    "    )\n",
    "    trainer = L.Trainer(\n",
    "        max_epochs=config.get(\"epochs\"),\n",
    "        accelerator=\"auto\",\n",
    "        devices=1,\n",
    "        logger=logger,\n",
    "    )\n",
    "    trainer.fit(model, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeea74002f402df7",
   "metadata": {},
   "source": [
    "Instantiate data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc31264725ea79e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, wandb.config[\"train_batch_size\"], shuffle=True)\n",
    "valid_loader = torch.utils.data.DataLoader(train, wandb.config[\"valid_batch_size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5710a3c3640df3",
   "metadata": {},
   "source": [
    "Execute training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0b043c76c9360d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "You are using the plain ModelCheckpoint callback. Consider using LitModelCheckpoint which with seamless uploading to Model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type               | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | fc1        | Linear             | 7.4 M  | train\n",
      "1 | fc2        | Linear             | 20.5 K | train\n",
      "2 | loss_fn    | CrossEntropyLoss   | 0      | train\n",
      "3 | _train_acc | MulticlassAccuracy | 0      | train\n",
      "4 | _valid_acc | MulticlassAccuracy | 0      | train\n",
      "----------------------------------------------------------\n",
      "7.4 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.4 M     Total params\n",
      "29.590    Total estimated model params size (MB)\n",
      "5         Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "203fe00958f045498fa201d14975f563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=127` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb_logger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[49], line 15\u001b[0m, in \u001b[0;36mtrain_classifier\u001b[0;34m(config, logger, train_loader, valid_loader)\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m CqaClassifier(\n\u001b[1;32m      4\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      5\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      6\u001b[0m     adam_epsilon\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam_epsilon\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      7\u001b[0m     weight_decay\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m )\n\u001b[1;32m      9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(\n\u001b[1;32m     10\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     11\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     devices\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     13\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlogger,\n\u001b[1;32m     14\u001b[0m )\n\u001b[0;32m---> 15\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:561\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshould_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 561\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:48\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     47\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     51\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:599\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    592\u001b[0m     download_model_from_registry(ckpt_path, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    593\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    595\u001b[0m     ckpt_path,\n\u001b[1;32m    596\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    597\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    598\u001b[0m )\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    602\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1012\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1007\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m   1009\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m   1011\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m-> 1012\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1054\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1053\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1054\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1056\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1083\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1080\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1083\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1085\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py:145\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py:437\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[1;32m    431\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[1;32m    436\u001b[0m )\n\u001b[0;32m--> 437\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:328\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 328\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    331\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py:412\u001b[0m, in \u001b[0;36mStrategy.validation_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 38\u001b[0m, in \u001b[0;36mCqaClassifier.validation_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvalidation_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, batch_idx):\n\u001b[0;32m---> 38\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     39\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n\u001b[1;32m     40\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(y_hat, y)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "train_classifier(wandb.config, wandb_logger, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80455405-b423-486a-8b99-8ec36f515f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "updating run config (2.6m)<br>  <strong style=\"color:red\">ERROR</strong> retrying HTTP 409: run a35wrs9i was previously created and deleted; try a new run name"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a92ceca9fa4119",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24831257099358aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3bb349b7a3afb091",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c163ba3cc6c0116d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "670529384ff1a832",
   "metadata": {},
   "source": [
    "# WordEmbeddings &rarr; RNN &rarr; Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdcb2c089c31f7d",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bae4a45e8fc91c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c23810130acebbab",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c727fc00f6873225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce68cd63697752b0",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008186ea326c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f241fe1cf1168f92",
   "metadata": {},
   "source": [
    "## Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fd1ce96fabd34a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
