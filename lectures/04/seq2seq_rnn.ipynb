{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20cf81d1",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence RNN\n",
    "In this exercise, we implement a sequence-to-sequence RNN (without attention)."
   ]
  },
  {
   "cell_type": "code",
   "id": "827d5ecf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:07:33.427418Z",
     "start_time": "2025-03-14T09:07:33.420119Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from thinc.backends.numpy_ops import lstm_forward_training"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "54b6923b",
   "metadata": {},
   "source": [
    "We first define our hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "id": "0b02ad78",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:07:34.039833Z",
     "start_time": "2025-03-14T09:07:34.031228Z"
    }
   },
   "source": [
    "embedding_dim = 10\n",
    "hidden_dim = 20\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "sequence_length = 5\n",
    "batch_size = 3"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "46cfc188",
   "metadata": {},
   "source": [
    "Create a bidirectional [`nn.LSTM`](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) with 2 layers."
   ]
  },
  {
   "cell_type": "code",
   "id": "88f1c683",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:07:41.414780Z",
     "start_time": "2025-03-14T09:07:41.409573Z"
    }
   },
   "source": "model = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=bidirectional)",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "4acc0342",
   "metadata": {},
   "source": [
    "We create an example input `x`."
   ]
  },
  {
   "cell_type": "code",
   "id": "89463769",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:07:45.077081Z",
     "start_time": "2025-03-14T09:07:45.060893Z"
    }
   },
   "source": [
    "x = torch.randn(sequence_length, batch_size, embedding_dim)"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "018a3a2c",
   "metadata": {},
   "source": [
    "What should the initial hidden and cell state be?"
   ]
  },
  {
   "cell_type": "code",
   "id": "aaf1dc1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:07:47.160839Z",
     "start_time": "2025-03-14T09:07:47.149029Z"
    }
   },
   "source": [
    "h0 = torch.zeros(num_layers * 2, batch_size, hidden_dim)\n",
    "c0 = torch.zeros(num_layers * 2, batch_size, hidden_dim)"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "a0e3373c",
   "metadata": {},
   "source": [
    "Now we run our LSTM. Look at the output. Explain each dimension of the output."
   ]
  },
  {
   "cell_type": "code",
   "id": "a18b7612",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:08:10.076897Z",
     "start_time": "2025-03-14T09:08:10.067373Z"
    }
   },
   "source": "output, (hn, cn) = model(x, (h0, c0))",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:08:58.003748Z",
     "start_time": "2025-03-14T09:08:57.994379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(output.shape)\n",
    "print(hn.shape)\n",
    "print(cn.shape)"
   ],
   "id": "849f91bf404cf650",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 40])\n",
      "torch.Size([4, 3, 20])\n",
      "torch.Size([4, 3, 20])\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "ab1d6d2d",
   "metadata": {},
   "source": [
    "All outputs are from the last (2nd) layer of the LSTM. If we want to have access to the hidden states of layer 1 as well, we have to run the `LSTMCell`s ourselves.\n",
    "\n",
    "When we take the above LSTM as the encoder, what is its output that serves as the input to the decoder?"
   ]
  },
  {
   "cell_type": "code",
   "id": "5386b9d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:30:05.606585Z",
     "start_time": "2025-03-14T09:30:05.596288Z"
    }
   },
   "source": [
    "encoder = model\n",
    "\n",
    "encoder_output = torch.cat([hn[2], hn[3]], dim=-1)\n",
    "print(encoder_output.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 40])\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "ec7afab4",
   "metadata": {},
   "source": [
    "Create a decoder LSTM with 2 layers. Why can't it be bidirectional as well? What is the hidden dimension of the decoder LSTM when you want to initialize it with the encoder output?"
   ]
  },
  {
   "cell_type": "code",
   "id": "373c7616",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:39:02.133376Z",
     "start_time": "2025-03-14T09:39:02.123029Z"
    }
   },
   "source": [
    "decoder_hidden_dim = hidden_dim * 2\n",
    "decoder = nn.LSTM(embedding_dim, decoder_hidden_dim, num_layers)"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "fab709dc",
   "metadata": {},
   "source": [
    "Run your decoder LSTM on an example sequence. Condition it with the encoder representation of the sequence. How do we get the correct shape for the initial hidden state?\n",
    "\n",
    "**Hint:** Take a look at [Torch's tensor operations](https://pytorch.org/docs/stable/tensors.html) and compare `Torch.repeat`, `Torch.repeat_interleave` and `Tensor.expand`."
   ]
  },
  {
   "cell_type": "code",
   "id": "56965f07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T09:59:07.061258Z",
     "start_time": "2025-03-14T09:59:07.049235Z"
    }
   },
   "source": [
    "output_seq_length = 7\n",
    "y = torch.randn(output_seq_length, batch_size, embedding_dim)\n",
    "h0_dec = encoder_output.unsqueeze(0).expand(num_layers, -1, -1) # only adds new view! does not copy the tensor\n",
    "# h0_dec = encoder_output.repeat(2, 1, 1)\n",
    "c0_dec = torch.zeros(num_layers, batch_size, decoder_hidden_dim)\n",
    "\n",
    "decoder_output, (hn_dec, cn_dec) = decoder(y, (h0_dec, c0_dec))\n",
    "\n",
    "print(decoder_output.shape)\n",
    "print(hn_dec.shape)\n",
    "print(cn_dec.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 3, 40])\n",
      "torch.Size([2, 3, 40])\n",
      "torch.Size([2, 3, 40])\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "id": "7d9ac2ab",
   "metadata": {},
   "source": [
    "In most RNNs, the final encoder hidden state is used as the first hidden state of the decoder RNN. In some variants, it has also been concatenated with the hidden state of the previous time step at each decoder time step. In PyTorch's `nn.LSTM` implementation, we cannot easily do that, so we would have to resort to the lower-level `nn.LSTMCell` class again.\n",
    "\n",
    "Put it all together in a seq2seq LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "id": "af981a96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T10:18:44.543409Z",
     "start_time": "2025-03-14T10:18:44.533426Z"
    }
   },
   "source": [
    "class Seq2seqLSTM(nn.Module):\n",
    "    \"\"\" Sequence-to-sequence LSTM. \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, num_encoder_layers, num_decoder_layers, bidirectional):\n",
    "        super().__init__()\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.bidirectional = bidirectional\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, num_encoder_layers, bidirectional=bidirectional)\n",
    "        self.decoder = nn.LSTM(embedding_dim, self.num_directions * hidden_dim, num_decoder_layers)\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        assert x.dim() == 3, \"Expected input of shape [sequence length, batch size, embedding dim]\"\n",
    "        batch_size = x.size(1)\n",
    "        \n",
    "        # Encoder\n",
    "        h0_en = torch.zeros(self.num_directions * self.encoder.num_layers, batch_size, self.encoder.hidden_size)\n",
    "        c0_en = torch.zeros(self.num_directions * self.encoder.num_layers, batch_size, self.encoder.hidden_size)\n",
    "        encoder_outputs, (hn_en, cn_en) = self.encoder(x, (h0_en, c0_en))\n",
    "        \n",
    "        # Decoder\n",
    "        encoder_output = torch.cat((hn_en[-2], hn_en[-1]), dim=-1) if self.bidirectional else hn_en[-1]\n",
    "        h0_dec = encoder_output.unsqueeze(0).expand(self.decoder.num_layers, -1, -1)\n",
    "        c0_dec = torch.zeros(self.decoder.num_layers, batch_size, self.decoder.hidden_size)\n",
    "        decoder_outputs, _ = self.decoder(y, (h0_dec, c0_dec))\n",
    "        return decoder_outputs\n",
    "        \n",
    "        "
   ],
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "id": "241dd1ad",
   "metadata": {},
   "source": [
    "Test your seq2seq LSTM with an input sequence `x` and a ground truth output sequence `y` that the decoder tries to predict."
   ]
  },
  {
   "cell_type": "code",
   "id": "74ef14d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-14T10:18:45.248227Z",
     "start_time": "2025-03-14T10:18:45.234223Z"
    }
   },
   "source": [
    "num_directions = 2 if bidirectional else 1\n",
    "decoder_hidden_dim = num_directions * hidden_dim\n",
    "seq2seq_lstm = Seq2seqLSTM(embedding_dim, hidden_dim, num_layers, num_layers, bidirectional)\n",
    "x = torch.randn(10, 2, embedding_dim)\n",
    "y = torch.randn(9, 2, embedding_dim)\n",
    "outputs = seq2seq_lstm(x, y)\n",
    "assert outputs.dim() == 3 and list(outputs.size()) == [9, 2, decoder_hidden_dim], \"Wrong output shape\""
   ],
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7f388a88ac14038f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
