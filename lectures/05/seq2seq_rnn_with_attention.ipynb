{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75df666b",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence RNN with Attention\n",
    "We will now add attention to our sequence-to-sequence RNN. There are several ways to incorporate the context vector $c$ into the RNN architecture:\n",
    "1. Add an additional term to the computation of the gates/states (i.e. treat it as an input just like $h_{t-1}$ and $x_t$). This was used in the original paper (Bahdanau et al, 2015), described in Appendix A.\n",
    "2. Concatenate it with the hidden state of the last time step $h_{t-1}$ and project the concatenation down from `enc_hidden_dim + dec_hidden_dim` to `dec_hidden_dim`.\n",
    "3. Concatenate it with the input $x_t$ and downproject it.\n",
    "\n",
    "We will use variant 2 in this exercise. We'll make our lives a bit easier by implementing a 1-layer decoder and working with a batch size of 1."
   ]
  },
  {
   "cell_type": "code",
   "id": "1eae6e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T08:30:40.901776Z",
     "start_time": "2025-03-20T08:30:35.621780Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "f420803d",
   "metadata": {},
   "source": [
    "Since we have to compute the context vector at every step, we can't use the high-level `nn.LSTM` interface by PyTorch. We first implement a decoder LSTM class that operates an `nn.LSTMCell`. We start with the `__init__` method where we initialize all parameters."
   ]
  },
  {
   "cell_type": "code",
   "id": "1a9841cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T09:13:19.846686Z",
     "start_time": "2025-03-20T09:13:19.833928Z"
    }
   },
   "source": [
    "class DecoderLSTMWithAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, enc_output_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.dec_hidden_dim = dec_hidden_dim\n",
    "        self.enc_output_dim = enc_output_dim\n",
    "        self.cell = nn.LSTMCell(input_dim, dec_hidden_dim)\n",
    "        self.project_down = nn.Linear(enc_output_dim + dec_hidden_dim, dec_hidden_dim)\n",
    "        self.W = nn.Linear(dec_hidden_dim, enc_output_dim, bias=False)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "808d9186",
   "metadata": {},
   "source": [
    "Add a `reset_parameters` method that initializes all parameters."
   ]
  },
  {
   "cell_type": "code",
   "id": "6e9deec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T09:13:20.748991Z",
     "start_time": "2025-03-20T09:13:20.742522Z"
    }
   },
   "source": [
    "def reset_parameters(self):\n",
    "    self.cell.reset_parameters()\n",
    "    self.project_down.reset_parameters()\n",
    "    self.W.reset_parameters()\n",
    "\n",
    "DecoderLSTMWithAttention.reset_parameters = reset_parameters"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "ada5ce41",
   "metadata": {},
   "source": [
    "Add a `forward` method that takes a sequence `y` and encoder hidden states `encoder_hidden_states` as input. `encoder_hidden_states` is a tensor of size `[sequence_length, encoder_output_dim]`, where `encoder_output_dim = num_directions * encoder_hidden_dim`. The `forward` method should call `compute_context_vector` that computes the attention-weighted context vector. We will implement it later."
   ]
  },
  {
   "cell_type": "code",
   "id": "dd58a594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T09:13:24.727583Z",
     "start_time": "2025-03-20T09:13:24.714404Z"
    }
   },
   "source": [
    "def forward(self, y, encoder_hidden_states):\n",
    "    hidden_state = torch.zeros(self.dec_hidden_dim)\n",
    "    cell_state = torch.zeros(self.dec_hidden_dim)\n",
    "    for y_i in y:\n",
    "        context_vector = self.compute_context_vector(hidden_state, encoder_hidden_states)\n",
    "        projected = self.project_down(torch.cat(hidden_state, context_vector))\n",
    "        hidden_state, cell_state = self.cell(y_i, (projected, cell_state))\n",
    "    return hidden_state\n",
    "    \n",
    "DecoderLSTMWithAttention.forward = forward"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "b5bc3b86",
   "metadata": {},
   "source": [
    "Now it's time to implement the `compute_context_vector` function. Its inputs are `previous_decoder_hidden_state` and `encoder_hidden_states`. Use either additive or multiplicative attention, as we saw it in the course. Extend the trainable parameters in your `__init__` method if necessary and initialize them in `reset_parameters`."
   ]
  },
  {
   "cell_type": "code",
   "id": "fb43d017",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T09:13:27.133115Z",
     "start_time": "2025-03-20T09:13:27.118644Z"
    }
   },
   "source": [
    "def compute_context_vector(self, previous_decoder_hidden_state, encoder_hidden_states):\n",
    "    def f_att(h_dec, h_enc):\n",
    "        return h_dec @ self.W @ h_enc\n",
    "    \n",
    "    a = torch.softmax(torch.tensor([f_att(previous_decoder_hidden_state, h_enc) for h_enc in encoder_hidden_states]), dim=0)\n",
    "    return sum(a * encoder_hidden_states)\n",
    "    \n",
    "DecoderLSTMWithAttention.compute_context_vector = compute_context_vector"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "8e81b83f",
   "metadata": {},
   "source": [
    "**Sequence-to-sequence model.** We will use the following hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba3db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typically, encoder/decoder hidden dimensions are the same,\n",
    "# but here we choose them differently to test our implementation.\n",
    "embedding_dim = 10\n",
    "enc_hidden_dim = 15\n",
    "dec_hidden_dim = 20\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "num_directions = 2 if bidirectional else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea07b9",
   "metadata": {},
   "source": [
    "Now we define the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbef75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2seqLSTMWithAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, enc_hidden_dim, num_enc_layers, bidirectional, dec_hidden_dim):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, y, h0, c0):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0e3038",
   "metadata": {},
   "source": [
    "Try your Module with an example input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8f6cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Seq2seqLSTMWithAttention(embedding_dim, enc_hidden_dim, num_layers, bidirectional, dec_hidden_dim)\n",
    "x = torch.randn(10, embedding_dim)\n",
    "y = torch.randn(8, embedding_dim)\n",
    "h0 = torch.zeros(num_layers * num_directions, enc_hidden_dim)\n",
    "c0 = torch.zeros(num_layers * num_directions, enc_hidden_dim)\n",
    "outputs, _ = model(x, y, h0, c0)\n",
    "assert list(outputs.shape) == [8, dec_hidden_dim], \"Wrong output shape\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d471622",
   "metadata": {},
   "source": [
    "Create a subclass of your decoder LSTM that implements the other type of attention (additive or multiplicative) that you haven't implemented above. What do you need to change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e999d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLSTMWithMultiplicativeAttention(DecoderLSTMWithAttention):\n",
    "    # or: DecoderLSTMWithAdditiveAttention\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb688ab",
   "metadata": {},
   "source": [
    "We can test our implementation with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7cca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_output_dim = enc_hidden_dim * num_directions\n",
    "# Uncomment the version you just implemented\n",
    "# model.decoder = DecoderLSTMWithAdditiveAttention(embedding_dim, enc_output_dim, dec_hidden_dim)\n",
    "# model.decoder = DecoderLSTMWithMultiplicativeAttention(embedding_dim, enc_output_dim, dec_hidden_dim)\n",
    "model.decoder.reset_parameters()\n",
    "outputs, _ = model(x, y, h0, c0)\n",
    "assert list(outputs.shape) == [8, dec_hidden_dim], \"Wrong output shape\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
