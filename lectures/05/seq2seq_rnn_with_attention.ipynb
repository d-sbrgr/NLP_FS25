{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75df666b",
   "metadata": {},
   "source": [
    "# Sequence-to-sequence RNN with Attention\n",
    "We will now add attention to our sequence-to-sequence RNN. There are several ways to incorporate the context vector $c$ into the RNN architecture:\n",
    "1. Add an additional term to the computation of the gates/states (i.e. treat it as an input just like $h_{t-1}$ and $x_t$). This was used in the original paper (Bahdanau et al, 2015), described in Appendix A.\n",
    "2. Concatenate it with the hidden state of the last time step $h_{t-1}$ and project the concatenation down from `enc_hidden_dim + dec_hidden_dim` to `dec_hidden_dim`.\n",
    "3. Concatenate it with the input $x_t$ and downproject it.\n",
    "\n",
    "We will use variant 2 in this exercise. We'll make our lives a bit easier by implementing a 1-layer decoder and working with a batch size of 1."
   ]
  },
  {
   "cell_type": "code",
   "id": "1eae6e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T09:56:33.498185Z",
     "start_time": "2025-03-20T09:56:33.486677Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "f420803d",
   "metadata": {},
   "source": [
    "Since we have to compute the context vector at every step, we can't use the high-level `nn.LSTM` interface by PyTorch. We first implement a decoder LSTM class that operates an `nn.LSTMCell`. We start with the `__init__` method where we initialize all parameters."
   ]
  },
  {
   "cell_type": "code",
   "id": "1a9841cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T10:06:06.767380Z",
     "start_time": "2025-03-20T10:06:06.758345Z"
    }
   },
   "source": [
    "class DecoderLSTMWithAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, enc_output_dim, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        self.dec_hidden_dim = dec_hidden_dim\n",
    "        self.enc_output_dim = enc_output_dim\n",
    "        self.cell = nn.LSTMCell(input_dim, dec_hidden_dim)\n",
    "        self.project_down = nn.Linear(enc_output_dim + dec_hidden_dim, dec_hidden_dim)\n",
    "        self.W = nn.Parameter(torch.zeros(dec_hidden_dim, enc_output_dim))"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "808d9186",
   "metadata": {},
   "source": [
    "Add a `reset_parameters` method that initializes all parameters."
   ]
  },
  {
   "cell_type": "code",
   "id": "6e9deec9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T10:06:07.489070Z",
     "start_time": "2025-03-20T10:06:07.476973Z"
    }
   },
   "source": [
    "def reset_parameters(self):\n",
    "    self.cell.reset_parameters()\n",
    "    self.project_down.reset_parameters()\n",
    "    nn.init.normal_(self.W, mean=0, std=1)\n",
    "\n",
    "DecoderLSTMWithAttention.reset_parameters = reset_parameters"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "ada5ce41",
   "metadata": {},
   "source": [
    "Add a `forward` method that takes a sequence `y` and encoder hidden states `encoder_hidden_states` as input. `encoder_hidden_states` is a tensor of size `[sequence_length, encoder_output_dim]`, where `encoder_output_dim = num_directions * encoder_hidden_dim`. The `forward` method should call `compute_context_vector` that computes the attention-weighted context vector. We will implement it later."
   ]
  },
  {
   "cell_type": "code",
   "id": "dd58a594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T10:06:08.291574Z",
     "start_time": "2025-03-20T10:06:08.275149Z"
    }
   },
   "source": [
    "def forward(self, y, encoder_hidden_states):\n",
    "    hidden_state = torch.zeros(self.dec_hidden_dim)\n",
    "    cell_state = torch.zeros_like(hidden_state)\n",
    "    outputs = []\n",
    "    for y_i in y:\n",
    "        context_vector = self.compute_context_vector(hidden_state, encoder_hidden_states)\n",
    "        projected = self.project_down(torch.cat((hidden_state, context_vector), dim=-1))\n",
    "        hidden_state, cell_state = self.cell(y_i, (projected, cell_state))\n",
    "        outputs.append(hidden_state)\n",
    "    return torch.stack(outputs), (hidden_state, cell_state)\n",
    "    \n",
    "DecoderLSTMWithAttention.forward = forward"
   ],
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "b5bc3b86",
   "metadata": {},
   "source": [
    "Now it's time to implement the `compute_context_vector` function. Its inputs are `previous_decoder_hidden_state` and `encoder_hidden_states`. Use either additive or multiplicative attention, as we saw it in the course. Extend the trainable parameters in your `__init__` method if necessary and initialize them in `reset_parameters`."
   ]
  },
  {
   "cell_type": "code",
   "id": "fb43d017",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T10:06:09.128631Z",
     "start_time": "2025-03-20T10:06:09.123287Z"
    }
   },
   "source": [
    "def compute_context_vector(self, previous_decoder_hidden_state, encoder_hidden_states):      \n",
    "    a = F.softmax(torch.tensor([previous_decoder_hidden_state @ self.W @ h_enc for h_enc in encoder_hidden_states]), dim=-1)\n",
    "    return a @ encoder_hidden_states\n",
    "    \n",
    "DecoderLSTMWithAttention.compute_context_vector = compute_context_vector"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "8e81b83f",
   "metadata": {},
   "source": [
    "**Sequence-to-sequence model.** We will use the following hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "id": "4ba3db54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T10:06:10.249908Z",
     "start_time": "2025-03-20T10:06:10.244173Z"
    }
   },
   "source": [
    "# Typically, encoder/decoder hidden dimensions are the same,\n",
    "# but here we choose them differently to test our implementation.\n",
    "embedding_dim = 10\n",
    "enc_hidden_dim = 15\n",
    "dec_hidden_dim = 20\n",
    "num_layers = 2\n",
    "bidirectional = True\n",
    "num_directions = 2 if bidirectional else 1"
   ],
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "edea07b9",
   "metadata": {},
   "source": [
    "Now we define the model."
   ]
  },
  {
   "cell_type": "code",
   "id": "fbef75cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T10:06:11.217355Z",
     "start_time": "2025-03-20T10:06:11.203913Z"
    }
   },
   "source": [
    "class Seq2seqLSTMWithAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, enc_hidden_dim, num_enc_layers, bidirectional, dec_hidden_dim):\n",
    "        super().__init__()\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        enc_output_dim = num_directions * enc_hidden_dim\n",
    "        self.encoder = nn.LSTM(embedding_dim, enc_hidden_dim, num_enc_layers, bidirectional=bidirectional)\n",
    "        self.decoder = DecoderLSTMWithAttention(embedding_dim, enc_output_dim, dec_hidden_dim)\n",
    "        \n",
    "        self.encoder.reset_parameters()\n",
    "        self.decoder.reset_parameters()\n",
    "\n",
    "    def forward(self, x, y, h0, c0):\n",
    "        encoder_hidden_states, (hn, cn) = self.encoder(x, (h0, c0))\n",
    "        return self.decoder(y, encoder_hidden_states)"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "aa0e3038",
   "metadata": {},
   "source": [
    "Try your Module with an example input."
   ]
  },
  {
   "cell_type": "code",
   "id": "6a8f6cb0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T10:06:12.491885Z",
     "start_time": "2025-03-20T10:06:12.430160Z"
    }
   },
   "source": [
    "model = Seq2seqLSTMWithAttention(embedding_dim, enc_hidden_dim, num_layers, bidirectional, dec_hidden_dim)\n",
    "x = torch.randn(10, embedding_dim)\n",
    "y = torch.randn(8, embedding_dim)\n",
    "h0 = torch.zeros(num_layers * num_directions, enc_hidden_dim)\n",
    "c0 = torch.zeros(num_layers * num_directions, enc_hidden_dim)\n",
    "outputs, _ = model(x, y, h0, c0)\n",
    "assert list(outputs.shape) == [8, dec_hidden_dim], \"Wrong output shape\""
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "4d471622",
   "metadata": {},
   "source": [
    "Create a subclass of your decoder LSTM that implements the other type of attention (additive or multiplicative) that you haven't implemented above. What do you need to change?"
   ]
  },
  {
   "cell_type": "code",
   "id": "e999d322",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T10:22:44.922281Z",
     "start_time": "2025-03-20T10:22:44.901098Z"
    }
   },
   "source": [
    "class DecoderLSTMWithAdditiveAttention(DecoderLSTMWithAttention):\n",
    "    # or: DecoderLSTMWithAdditiveAttention\n",
    "    def __init__(self, input_dim, enc_output_dim, dec_hidden_dim):\n",
    "        super().__init__(input_dim, enc_output_dim, dec_hidden_dim)\n",
    "        self.W = nn.Parameter(torch.zeros(dec_hidden_dim, dec_hidden_dim))\n",
    "        self.U = nn.Parameter(torch.zeros(dec_hidden_dim, enc_output_dim))\n",
    "        self.v = nn.Parameter(torch.zeros(dec_hidden_dim))\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        super().reset_parameters()\n",
    "        nn.init.uniform_(self.U)\n",
    "        nn.init.uniform_(self.W)\n",
    "        nn.init.uniform_(self.v)\n",
    "        \n",
    "    def compute_context_vector(self, previous_decoder_hidden_state, encoder_hidden_states):      \n",
    "        a = F.softmax(torch.tensor([self.v @ F.tanh(self.W @ previous_decoder_hidden_state + self.U @ h_enc) for h_enc in encoder_hidden_states]), dim=-1)\n",
    "        return a @ encoder_hidden_states"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "ecb688ab",
   "metadata": {},
   "source": [
    "We can test our implementation with the code below."
   ]
  },
  {
   "cell_type": "code",
   "id": "d7b7cca1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T10:22:46.417520Z",
     "start_time": "2025-03-20T10:22:46.379442Z"
    }
   },
   "source": [
    "enc_output_dim = enc_hidden_dim * num_directions\n",
    "# Uncomment the version you just implemented\n",
    "model.decoder = DecoderLSTMWithAdditiveAttention(embedding_dim, enc_output_dim, dec_hidden_dim)\n",
    "# model.decoder = DecoderLSTMWithMultiplicativeAttention(embedding_dim, enc_output_dim, dec_hidden_dim)\n",
    "model.decoder.reset_parameters()\n",
    "outputs, _ = model(x, y, h0, c0)\n",
    "assert list(outputs.shape) == [8, dec_hidden_dim], \"Wrong output shape\""
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e9ab9e53aa55c2c2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
