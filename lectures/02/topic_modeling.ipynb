{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d81604-025d-4fe1-a130-6a978f5ba135",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "In this exercise, we will do topic modeling with gensim. Use the [topics and transformations tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html) as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e45876ae-0f77-4bf8-8da4-b18618005327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6efd1",
   "metadata": {},
   "source": [
    "For tokenizing words and stopword removal, download the NLTK punkt tokenizer and stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "edf524f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee84f40-20bf-47da-b0b4-a0ff28f9b5cd",
   "metadata": {},
   "source": [
    "First, we load the [Lee Background Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf) included with gensim that contains 300 news articles of the Australian Broadcasting Corporation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24d72e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "train_file = datapath('lee_background.cor')\n",
    "articles_orig = open(train_file).read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2e56f",
   "metadata": {},
   "source": [
    "Preprocess the text by lowercasing, removing stopwords, stemming, and removing rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88a870af-9f6b-43ea-940f-558e9a21bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stopword list\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords = stopwords | {'\\\"', '\\'', '\\'\\'', '`', '``', '\\'s'}\n",
    "\n",
    "# initialize stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess(article):\n",
    "    # tokenize\n",
    "    article = nltk.word_tokenize(article)\n",
    "\n",
    "    # lowercase all words\n",
    "    article = [word.lower() for word in article]\n",
    "\n",
    "    # remove stopwords\n",
    "    article = [word for word in article if word not in stopwords]\n",
    "\n",
    "    # optional: stem\n",
    "    article = [stemmer.stem(word) for word in article]\n",
    "    return article\n",
    "\n",
    "articles = [preprocess(article) for article in articles_orig]\n",
    "\n",
    "# create the dictionary and corpus objects that gensim uses for topic modeling\n",
    "dictionary = gensim.corpora.Dictionary(articles)\n",
    "\n",
    "# remove words that occur in less than 2 documents, or more than 50% of documents\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "temp = dictionary[0]  # load the dictionary by calling it once\n",
    "corpus_bow = [dictionary.doc2bow(article) for article in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ae61a",
   "metadata": {},
   "source": [
    "\n",
    "Now we create a TF-IDF model and transform the corpus into TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fab13db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.045055071138122856), (1, 0.048886979465828866), (2, 0.09375399876622974), (3, 0.06781504448328635), (4, 0.08078452162475803), (5, 0.10134065654795091), (6, 0.058730707569876556), (7, 0.07269531841603805), (8, 0.03468336276559477), (9, 0.07660928255691886), (10, 0.06936672553118954), (11, 0.06363980541544716), (12, 0.14639572768607376), (13, 0.06561120606131572), (14, 0.05734407327660557), (15, 0.21959359152911065), (16, 0.0861673409845086), (17, 0.03352561206466457), (18, 0.05163007712346614), (19, 0.0861673409845086), (20, 0.04649508920613628), (21, 0.08078452162475803), (22, 0.058730707569876556), (23, 0.044374596135133865), (24, 0.048886979465828866), (25, 0.053711220021490196), (26, 0.04725890956009348), (27, 0.06185645660730057), (28, 0.05163007712346614), (29, 0.0861673409845086), (30, 0.056053147633726), (31, 0.06185645660730057), (32, 0.14639572768607376), (33, 0.04725890956009348), (34, 0.06022838670156517), (35, 0.06363980541544716), (36, 0.07660928255691886), (37, 0.07660928255691886), (38, 0.06185645660730057), (39, 0.13122241212263144), (40, 0.09375399876622974), (41, 0.05264172891984401), (42, 0.3350087216027434), (43, 0.06185645660730057), (44, 0.04187609020034293), (45, 0.08671983917661366), (46, 0.0861673409845086), (47, 0.04247014010577541), (48, 0.06781504448328635), (49, 0.09375399876622974), (50, 0.09375399876622974), (51, 0.1723346819690172), (52, 0.15321856511383772), (53, 0.1806851601046955), (54, 0.1331237884054016), (55, 0.06363980541544716), (56, 0.18750799753245947), (57, 0.10326015424693227), (58, 0.07660928255691886), (59, 0.07660928255691886), (60, 0.0861673409845086), (61, 0.0430836704922543), (62, 0.1356300889665727), (63, 0.07031355041807727), (64, 0.06781504448328635), (65, 0.03678793835341271), (66, 0.05163007712346614), (67, 0.044374596135133865), (68, 0.045055071138122856), (69, 0.050670328273975454), (70, 0.05163007712346614), (71, 0.02980491357610684), (72, 0.0430836704922543), (73, 0.07319786384303688), (74, 0.1659534940873187), (75, 0.04805524252058899), (76, 0.09375399876622974), (77, 0.03967225177837232), (78, 0.04263991055430013), (79, 0.03678793835341271), (80, 0.06022838670156517), (81, 0.15321856511383772), (82, 0.07031355041807727), (83, 0.058730707569876556), (84, 0.22982784767075656), (85, 0.056053147633726), (86, 0.08078452162475803), (87, 0.058730707569876556), (88, 0.06561120606131572), (89, 0.07031355041807727), (90, 0.04187609020034293), (91, 0.06781504448328635), (92, 0.14308257109546987), (93, 0.07660928255691886), (94, 0.10969113468362927), (95, 0.05734407327660557), (96, 0.024966735589369113), (97, 0.035917502324357156), (98, 0.06185645660730057), (99, 0.07031355041807727), (100, 0.058730707569876556), (101, 0.11901675533511694), (102, 0.058730707569876556), (103, 0.08078452162475803), (104, 0.030428671347565647), (105, 0.1806851601046955), (106, 0.05264172891984401), (107, 0.056053147633726), (108, 0.028330844542635996), (109, 0.08078452162475803), (110, 0.17487200975076178), (111, 0.112106295267452), (112, 0.04576123042840485), (113, 0.11468814655321113), (114, 0.024966735589369113)]\n"
     ]
    }
   ],
   "source": [
    "tfidf_model = gensim.models.TfidfModel(corpus_bow)\n",
    "corpus_tfidf = tfidf_model[corpus_bow] \n",
    "print(corpus_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24df8cb",
   "metadata": {},
   "source": [
    "Now we train an [LDA model](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html) with 10 topics on the TF-IDF corpus. Save it to a variable `model_lda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ded6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "\n",
    "model_lda = gensim.models.LdaModel(\n",
    "    corpus=corpus_tfidf,\n",
    "    id2word=dictionary,\n",
    "    num_topics=num_topics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91845654",
   "metadata": {},
   "source": [
    "Let's inspect the first 5 topics of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca3a357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9,\n",
       "  '0.003*\"palestinian\" + 0.003*\"reid\" + 0.002*\"mr\" + 0.002*\"arafat\" + 0.002*\"farmer\" + 0.002*\"road\" + 0.002*\"isra\" + 0.002*\"sharon\" + 0.002*\"petrol\" + 0.001*\"gaza\"'),\n",
       " (8,\n",
       "  '0.003*\"test\" + 0.002*\"south\" + 0.002*\"lee\" + 0.002*\"australian\" + 0.002*\"sydney\" + 0.002*\"wicket\" + 0.002*\"india\" + 0.002*\"africa\" + 0.002*\"compani\" + 0.002*\"forc\"'),\n",
       " (7,\n",
       "  '0.002*\"fire\" + 0.002*\"hospit\" + 0.002*\"claim\" + 0.002*\"australian\" + 0.002*\"taliban\" + 0.002*\"afghanistan\" + 0.002*\"line\" + 0.002*\"govern\" + 0.002*\"boat\" + 0.002*\"south\"'),\n",
       " (6,\n",
       "  '0.002*\"year\" + 0.002*\"rate\" + 0.002*\"peopl\" + 0.002*\"bank\" + 0.002*\"cut\" + 0.002*\"new\" + 0.002*\"1996\" + 0.001*\"man\" + 0.001*\"interest\" + 0.001*\"island\"'),\n",
       " (3,\n",
       "  '0.002*\"economi\" + 0.002*\"palestinian\" + 0.002*\"qanta\" + 0.002*\"rate\" + 0.002*\"mr\" + 0.002*\"arafat\" + 0.002*\"isra\" + 0.002*\"child\" + 0.002*\"think\" + 0.002*\"australia\"')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lda.print_topics(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ce453",
   "metadata": {},
   "source": [
    "We see the 5 topics with the highest importance. For each topic, the 10 most important words are shown, together with their coefficient of \"alignment\" to the topic.\n",
    "\n",
    "## Document Similarity\n",
    "We now use our LDA model to compare the similarity of new documents (*queries*) to documents in our collection.\n",
    "\n",
    "First, create an index of the news articles in our corpus. Use the `MatrixSimilarity` transformation as described in gensim's [similarity queries tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4eb44cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = gensim.similarities.MatrixSimilarity(model_lda[corpus_tfidf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b2c1f",
   "metadata": {},
   "source": [
    "Now, write a function that takes a query string as input and returns the LDA representation for it. Make sure to apply the same preprocessing as we did to the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf9dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b77753be",
   "metadata": {},
   "source": [
    "Print the top 5 most similar documents, together with their similarities, using your index created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696f2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56e05dba",
   "metadata": {},
   "source": [
    "Run your code again, now training an LDA model with 100 topics. Do you see a qualitative difference in the top-5 most similar documents?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
