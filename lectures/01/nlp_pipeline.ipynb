{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40427979",
   "metadata": {},
   "source": [
    "# NLP Pipeline\n",
    "We will learn some basic operations of the NLP pipelines with the help of two libraries, NLTK and spaCy. These will prove useful in your project for tasks such as preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144162dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q nltk matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8333d6fa",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "We will use [Chapter 3 of the introductory manual of NLTK](https://www.nltk.org/book/ch03.html) to solve the following exercises.\n",
    "\n",
    "First, use the interactive downloader to get the *gutenberg* book corpus (in the *Corpora* tab). Take a moment to admire the amazing user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d021c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9da622",
   "metadata": {},
   "source": [
    "NLTK automatically saves its files in the `nltk_data` folder in your home directory. (Windows users: Maybe it's in your `Documents`?)\n",
    "\n",
    "We load the raw text of the first book in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977b230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "raw_text = gutenberg.raw('austen-emma.txt')\n",
    "print(len(raw_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb99b02",
   "metadata": {},
   "source": [
    "To use nltk's tokenization, we need to download the `punkt_tab` resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779c157c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb59b1d",
   "metadata": {},
   "source": [
    "Count the number of words and sentences in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015dbbf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef55d386",
   "metadata": {},
   "source": [
    "- Lowercase all words. \n",
    "- Count the number of unique lower-cased words in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2028dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f5a45a3",
   "metadata": {},
   "source": [
    "The total number of words is also called *tokens*, and the unique words are the *types*. (This is the same distinction as instance vs. class in software engineering.)\n",
    "\n",
    "We now perform *stemming* on the first 50 words of the text. We use the PorterStemmer (example usage [here](https://www.nltk.org/howto/stem.html)). Plot only the pairs of words that have changed after stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae02b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "522cf088",
   "metadata": {},
   "source": [
    "Find the 10 most frequent words, together with their counts, using the `FreqDist` object from NLTK. Save it to a variable named `fdist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425c26c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "abfc5cff",
   "metadata": {},
   "source": [
    "Run the below command to see the word counts vs. the words in the frequency distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdd446",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.plot(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f776c7b9",
   "metadata": {},
   "source": [
    "This curve is characteristic of any natural (= human) language, and is known as [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law). It states that the frequency of a word is inversely proportional to its rank, i.e.:\n",
    "$$\\text{frequency}(x) \\propto \\frac{1}{\\text{rank}(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3444a5e9",
   "metadata": {},
   "source": [
    "## Hugging Face\n",
    "\n",
    "With Transformers (we will get to know them in more detail later in the course), tokenization has become part of the model itself. As opposed to `word_tokenize` in NLTK, Transformers use BPE tokenization.\n",
    "\n",
    "We first install Hugging Face's `transformers` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac74dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e8318c",
   "metadata": {},
   "source": [
    "Use the [model page of the base-uncased version of BERT](https://huggingface.co/google-bert/bert-base-uncased) to initialize a `BertTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545492bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b177d372",
   "metadata": {},
   "source": [
    "We look at the first sentence of Jane Austen's Emma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2eca187",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = nltk.sent_tokenize(raw_text)[0].split('\\n\\n\\n')[-1].replace('\\n', ' ')\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711cc14b",
   "metadata": {},
   "source": [
    "Encode the sentence. Look at the outputs of the following functions:\n",
    "- `tokenizer(sentence)`\n",
    "- `tokenizer.encode(sentence)`\n",
    "- `tokenizer.tokenize(sentence)`\n",
    "- `tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentence))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99ddc96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b7629a8",
   "metadata": {},
   "source": [
    "**Decoding.** Check out the various ways of decoding: `.decode`, `.convert_ids_to_tokens`, `.convert_tokens_to_string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad3b807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8673fdd",
   "metadata": {},
   "source": [
    "**Tokenization differences.** Compare the tokenization of NLTK, bert-base-uncased and [t5-small](https://huggingface.co/google-t5/t5-small)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc8fb9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60559c74",
   "metadata": {},
   "source": [
    "Now `encode` and `decode` the sentence with all 3 tokenizers. Is the encoding/decoding lossless, i.e. do we recover the original sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851fe7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89ab450c",
   "metadata": {},
   "source": [
    "## spaCy\n",
    "We will use the [spaCy 101 guide](https://spacy.io/usage/spacy-101) to familiarize ourselves with its capabilities. First we install and download the small core model for English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8500c142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "%pip install setuptools wheel\n",
    "%pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce37219",
   "metadata": {},
   "source": [
    "*(You may have to restart your kernel after installing spacy.)*\n",
    "\n",
    "We can now load the NLP pipeline from the small model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7c3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "pipeline = spacy.load('en_core_web_sm')\n",
    "doc = pipeline('Apple is looking at buying U.K. startup for $1 billion.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebf94fec",
   "metadata": {},
   "source": [
    "The `doc` object has now tokenized our sentence and ran the NLP pipeline on it, as shown in this image:\n",
    "![spacy pipeline](https://spacy.io/images/pipeline.svg)\n",
    "\n",
    "Use the 101 guide to display the following information about each token.\n",
    "- Text: The original word text.\n",
    "- Lemma: The base form of the word.\n",
    "- POS: The simple [UPOS](https://universaldependencies.org/u/pos/) part-of-speech tag.\n",
    "- Tag: The detailed part-of-speech tag.\n",
    "- Dep: Syntactic dependency, i.e. the relation between tokens.\n",
    "- Shape: The word shape â€“ capitalization, punctuation, digits.\n",
    "- is stopword: Is the token part of a stop list, i.e. the most common words of the language?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd87b5d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41f3326a",
   "metadata": {},
   "source": [
    "Looking at the named entities in `doc`, print the following information:\n",
    "- The token's text\n",
    "- The start index of the named entity\n",
    "- Its end index\n",
    "- The NER label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d7418a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "decfbf10",
   "metadata": {},
   "source": [
    "**Question:** Search the internet for a description of the entity labels `FAC`, `ORG`, `GPE` and `LOC`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672d6e3e",
   "metadata": {},
   "source": [
    "**Answer:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
